# -*- coding: utf-8 -*-
"""Fake_news_BERT_TF_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WjXE8zT7GApd8OzkMadxOThstYUipFwc

## 1.1 Chargement des Données
"""

import pandas as pd

# URL du dataset
url = 'https://fnd-jedha-project.s3.eu-west-3.amazonaws.com/0_WELFake_workbase.csv'

# Chargement du dataset
df = pd.read_csv(url)

# Affichage des premières lignes du dataset
print(df.head())

"""## 1.2 Prétraitement des Données"""

import re

# Fonction de suppression des URLs
def remove_urls(text):
    # Supprimer les URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    return text

# Appliquer la fonction de suppression des URLs aux textes
df['message'] = df['message'].apply(remove_urls)

# Affichage des premières lignes du dataset nettoyé
print(df.head())

"""## 1.3 Séparation des Données"""

from sklearn.model_selection import train_test_split

# Séparation des données
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['message'], df['label'], test_size=0.2, random_state=42)

print(f'Training set size: {len(train_texts)}')
print(f'Validation set size: {len(val_texts)}')

"""## 2. Tokenisation des Textes
---


"""

from transformers import BertTokenizer
import tensorflow as tf

# Chargement du tokenizer BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Fonction de tokenisation
def encode_data(texts, tokenizer, max_len):
    input_ids = []
    attention_masks = []

    for text in texts:
        encoded_dict = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=max_len,
            pad_to_max_length=True,
            return_attention_mask=True,
            return_tensors='tf',
        )
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])

    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_masks)

# Tokenisation des textes
max_len = 128
train_input_ids, train_attention_masks = encode_data(train_texts, tokenizer, max_len)
val_input_ids, val_attention_masks = encode_data(val_texts, tokenizer, max_len)

# Affichage des dimensions des tenseurs
print(f'Train input_ids shape: {train_input_ids.shape}')
print(f'Train attention_masks shape: {train_attention_masks.shape}')
print(f'Val input_ids shape: {val_input_ids.shape}')
print(f'Val attention_masks shape: {val_attention_masks.shape}')

"""## 2.1Tokenisation des Textes (Correction)

"""

from transformers import BertTokenizer
import tensorflow as tf

# Chargement du tokenizer BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Fonction de tokenisation
def encode_data(texts, tokenizer, max_len):
    input_ids = []
    attention_masks = []

    for text in texts:
        encoded_dict = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=max_len,
            pad_to_max_length=True,
            return_attention_mask=True,
            return_tensors='tf',
            truncation=True  # Ajout de l'argument truncation
        )
        input_ids.append(encoded_dict['input_ids'][0])  # Suppression de la dimension supplémentaire
        attention_masks.append(encoded_dict['attention_mask'][0])  # Suppression de la dimension supplémentaire

    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_masks)

# Tokenisation des textes
max_len = 128
train_input_ids, train_attention_masks = encode_data(train_texts, tokenizer, max_len)
val_input_ids, val_attention_masks = encode_data(val_texts, tokenizer, max_len)

# Affichage des dimensions des tenseurs
print(f'Train input_ids shape: {train_input_ids.shape}')
print(f'Train attention_masks shape: {train_attention_masks.shape}')
print(f'Val input_ids shape: {val_input_ids.shape}')
print(f'Val attention_masks shape: {val_attention_masks.shape}')

"""## 3. Création du Modèle BERT"""

from transformers import TFBertModel
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Model

class NewsClassifier(tf.keras.Model):
    def __init__(self, n_classes):
        super(NewsClassifier, self).__init__()
        self.bert = TFBertModel.from_pretrained('bert-base-uncased')
        self.dropout = Dropout(0.3)
        self.classifier = Dense(n_classes, activation='softmax')

    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):
        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)
        pooled_output = outputs[1]
        pooled_output = self.dropout(pooled_output, training=training)
        return self.classifier(pooled_output)

# Création du modèle
model = NewsClassifier(n_classes=2)

# Affichage de la structure du modèle
model.summary()

"""## Création du Modèle BERT (Affichage de la Structure Complète)"""

# Affichage de la structure complète du modèle BERT
model.bert.summary()

from transformers import TFDistilBertModel
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Model

class NewsClassifier(tf.keras.Model):
    def __init__(self, n_classes):
        super(NewsClassifier, self).__init__()
        self.bert = TFDistilBertModel.from_pretrained('distilbert-base-uncased')
        self.dropout = Dropout(0.3)
        self.classifier = Dense(n_classes)  # Suppression de l'activation softmax

    def call(self, inputs, attention_mask=None, training=False):
        outputs = self.bert(inputs, attention_mask=attention_mask)
        pooled_output = outputs[0][:, 0, :]  # Utiliser la sortie du premier token [CLS]
        pooled_output = self.dropout(pooled_output, training=training)
        return self.classifier(pooled_output)

# Création du modèle
model = NewsClassifier(n_classes=2)

# Compilation du modèle avec un taux d'apprentissage plus élevé
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Création des datasets TensorFlow
train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': train_input_ids, 'attention_mask': train_attention_masks}, train_labels))
val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_input_ids, 'attention_mask': val_attention_masks}, val_labels))

# Batching et shuffling des datasets avec une taille de batch plus petite
train_dataset = train_dataset.shuffle(1000).batch(16)
val_dataset = val_dataset.batch(16)

# Entraînement du modèle avec plus d'époques
model.fit(train_dataset, epochs=10, validation_data=val_dataset)

# Évaluation du modèle
val_loss, val_accuracy = model.evaluate(val_dataset)
print(f'Validation Loss: {val_loss}')
print(f'Validation Accuracy: {val_accuracy}')

"""## Sauvegarde du Modèle en Format .keras"""

import os

# Création du répertoire s'il n'existe pas
os.makedirs('fake_news_detector', exist_ok=True)

# Sauvegarde du modèle en format .keras
model.save('fake_news_detector/model.keras')

"""## Sauvegarde des Poids du Modèle"""

# Sauvegarde des poids du modèle
model.save_weights('fake_news_detector/model_weights.weights.h5')

"""## Vérification des Fichiers Sauvegardés"""

import os

# Lister les fichiers dans le répertoire fake_news_detector
files = os.listdir('fake_news_detector')
print(files)

"""## Téléchargement des Fichiers Sauvegardés sur pc"""

from google.colab import files

# Téléchargement du fichier du modèle
files.download('fake_news_detector/model.keras')

# Téléchargement du fichier des poids du modèle
files.download('fake_news_detector/model_weights.weights.h5')

import os

# Lister les fichiers dans le répertoire fake_news_detector
files = os.listdir('fake_news_detector')
print(files)

import os

# Lister les fichiers dans le répertoire courant
files = os.listdir('.')
print(files)

